{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S2DC Reinforcement Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apwvt/S2DC-RL-Project/blob/self-play/S2DC%20Reinforcement%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsCC7TT8ujJF"
      },
      "source": [
        "**Pull Project Code**\r\n",
        "\r\n",
        "---\r\n",
        "We first get the latest version of the project code.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtaudn-U6THQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d66b157-452c-41a7-ded4-121594ceba07"
      },
      "source": [
        "from os import path\r\n",
        "\r\n",
        "% cd /content/\r\n",
        "\r\n",
        "if path.exists(\"S2DC-RL-Project\"):\r\n",
        "  ! cd S2DC-RL-Project && git pull\r\n",
        "else:\r\n",
        "  ! git clone https://github.com/apwvt/S2DC-RL-Project.git"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/apwvt/S2DC-RL-Project\n",
            "   06f4e40..5437b5a  self-play  -> origin/self-play\n",
            "Updating 06f4e40..5437b5a\n",
            "Fast-forward\n",
            " S2DC Reinforcement Learning.ipynb | 180 \u001b[32m+++++++++++++++++++++++++++++++\u001b[m\u001b[31m-------\u001b[m\n",
            " 1 file changed, 150 insertions(+), 30 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tac080_0yU8P"
      },
      "source": [
        "# We add the repository folder to the module path.\r\n",
        "import sys\r\n",
        "sys.path.insert(1, \"/content/S2DC-RL-Project\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6ir-RUx8VKA"
      },
      "source": [
        "**Mount Drive**\r\n",
        "\r\n",
        "---\r\n",
        "We mount a shared drive to have a place to save checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1l0dLac8eHw",
        "outputId": "fde1f6bb-715e-4978-d9dd-4fd8dc4eadb7"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/Drive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/Drive; to attempt to forcibly remount, call drive.mount(\"/content/Drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSnPpCSy-AE3",
        "outputId": "3536a054-b5b0-483b-e476-7286bc015daa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%set_env LOGFOLDER=/content/Drive/Shareddrives/CS_ML_ENV/colab_env/logs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: LOGFOLDER=/content/Drive/Shareddrives/CS_ML_ENV/colab_env/logs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYWAuU3qNSL4"
      },
      "source": [
        "**Setup Dependencies**\r\n",
        "\r\n",
        "---\r\n",
        "We install our repository as a local package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJSCibvZNnmj"
      },
      "source": [
        "# Avoid dumping loads of worthless output all over the screen\r\n",
        "%%capture\r\n",
        "\r\n",
        "% cd S2DC-RL-Project\r\n",
        "! pip install -e .  # Local module\r\n",
        "! pip install ray   # Distributed computing package\r\n",
        "! pip install pettingzoo # Environment support\r\n",
        "! pip install pettingzoo[magent] # Multi-agent environments\r\n",
        "! pip install tensorboard"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMfbvc8lx9oI"
      },
      "source": [
        "**Run Project Code**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "Any project files can be loaded from here and used in programs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51Tn-O-mLto",
        "outputId": "540f2da7-30ce-40d8-b901-9b79656f6b18"
      },
      "source": [
        "# Load the TensorBoard notebook extension\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp4_oA6wSX9n",
        "outputId": "f5ad1617-b7b9-4d5e-b5ea-6a7b5d2e8fa5"
      },
      "source": [
        "# Switch to the git branch to use\r\n",
        "! git checkout self-play"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M\tmuzero_collab/muzero.py\n",
            "Already on 'self-play'\n",
            "Your branch is up to date with 'origin/self-play'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dMkAUD7_yFy"
      },
      "source": [
        "# Set up M0 config\r\n",
        "import os\r\n",
        "import datetime\r\n",
        "import torch\r\n",
        "\r\n",
        "class BattleMuZeroConfig:\r\n",
        "    def __init__(self):\r\n",
        "        # More information is available here: https://github.com/werner-duvaud/muzero-general/wiki/Hyperparameter-Optimization\r\n",
        "\r\n",
        "        self.seed = 0x1BADB007  # Seed for numpy, torch and the game\r\n",
        "        self.max_num_gpus = None  # Fix the maximum number of GPUs to use. It's usually faster to use a single GPU (set it to 1) if it has enough memory. None will use every GPUs available\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        ### Game\r\n",
        "        self.observation_shape = (13, 13, 41)  # Dimensions of the game observation, must be 3D (channel, height, width). For a 1D array, please reshape it to (1, 1, length of array)\r\n",
        "        self.action_space = list(range(21))  # Fixed list of all possible actions. You should only edit the length\r\n",
        "        self.players = list(range(1))  # List of players. You should only edit the length\r\n",
        "        self.stacked_observations = 5  # Number of previous observations and previous actions to add to the current observation\r\n",
        "\r\n",
        "        # Evaluate\r\n",
        "        self.muzero_player = 0  # Turn Muzero begins to play (0: MuZero plays first, 1: MuZero plays second)\r\n",
        "        self.opponent = None  # Hard coded agent that MuZero faces to assess his progress in multiplayer games. It doesn't influence training. None, \"random\" or \"expert\" if implemented in the Game class\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        ### Self-Play\r\n",
        "        self.num_workers = 1  # Number of simultaneous threads/workers self-playing to feed the replay buffer\r\n",
        "        self.selfplay_on_gpu = False\r\n",
        "        self.max_moves = 500  # Maximum number of moves if game is not finished before\r\n",
        "        self.num_simulations = 50  # Number of future moves self-simulated\r\n",
        "        self.discount = 0.997  # Chronological discount of the reward\r\n",
        "        self.temperature_threshold = None  # Number of moves before dropping the temperature given by visit_softmax_temperature_fn to 0 (ie selecting the best action). If None, visit_softmax_temperature_fn is used every time\r\n",
        "\r\n",
        "        # Root prior exploration noise\r\n",
        "        self.root_dirichlet_alpha = 0.25\r\n",
        "        self.root_exploration_fraction = 0.25\r\n",
        "\r\n",
        "        # UCB formula\r\n",
        "        self.pb_c_base = 19652\r\n",
        "        self.pb_c_init = 1.25\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        ### Network\r\n",
        "        self.network = \"fullyconnected\"  # \"resnet\" / \"fullyconnected\"\r\n",
        "        self.support_size = 10  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range of -support_size to support_size. Choose it so that support_size <= sqrt(max(abs(discounted reward)))\r\n",
        "     \r\n",
        "        # Residual Network\r\n",
        "        self.downsample = False  # Downsample observations before representation network, False / \"CNN\" (lighter) / \"resnet\" (See paper appendix Network Architecture)\r\n",
        "        self.blocks = 1  # Number of blocks in the ResNet\r\n",
        "        self.channels = 2  # Number of channels in the ResNet\r\n",
        "        self.reduced_channels_reward = 2  # Number of channels in reward head\r\n",
        "        self.reduced_channels_value = 2  # Number of channels in value head\r\n",
        "        self.reduced_channels_policy = 2  # Number of channels in policy head\r\n",
        "        self.resnet_fc_reward_layers = []  # Define the hidden layers in the reward head of the dynamic network\r\n",
        "        self.resnet_fc_value_layers = []  # Define the hidden layers in the value head of the prediction network\r\n",
        "        self.resnet_fc_policy_layers = []  # Define the hidden layers in the policy head of the prediction network\r\n",
        "\r\n",
        "        # Fully Connected Network\r\n",
        "        self.encoding_size = 8\r\n",
        "        self.fc_representation_layers = []  # Define the hidden layers in the representation network\r\n",
        "        self.fc_dynamics_layers = [16]  # Define the hidden layers in the dynamics network\r\n",
        "        self.fc_reward_layers = [16]  # Define the hidden layers in the reward network\r\n",
        "        self.fc_value_layers = [16]  # Define the hidden layers in the value network\r\n",
        "        self.fc_policy_layers = [16]  # Define the hidden layers in the policy network\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        ### Training\r\n",
        "        self.results_path = os.path.join(os.environ.get(\"LOGFOLDER\"), \"battle\", datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\"))  # Path to store the model weights and TensorBoard logs\r\n",
        "        self.save_model = True  # Save the checkpoint in results_path as model.checkpoint\r\n",
        "        self.training_steps = 5000  # Total number of training steps (ie weights update according to a batch)\r\n",
        "        self.batch_size = 128  # Number of parts of games to train on at each training step\r\n",
        "        self.checkpoint_interval = 10  # Number of training steps before using the model for self-playing\r\n",
        "        self.value_loss_weight = 1  # Scale the value loss to avoid overfitting of the value function, paper recommends 0.25 (See paper appendix Reanalyze)\r\n",
        "        self.train_on_gpu = torch.cuda.is_available()  # Train on GPU if available\r\n",
        "\r\n",
        "        self.optimizer = \"Adam\"  # \"Adam\" or \"SGD\". Paper uses SGD\r\n",
        "        self.weight_decay = 1e-4  # L2 weights regularization\r\n",
        "        self.momentum = 0.9  # Used only if optimizer is SGD\r\n",
        "\r\n",
        "        # Exponential learning rate schedule\r\n",
        "        self.lr_init = 0.02  # Initial learning rate\r\n",
        "        self.lr_decay_rate = 0.9  # Set it to 1 to use a constant learning rate\r\n",
        "        self.lr_decay_steps = 1000\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        ### Replay Buffer\r\n",
        "        self.replay_buffer_size = 500  # Number of self-play games to keep in the replay buffer\r\n",
        "        self.num_unroll_steps = 10  # Number of game moves to keep for every batch element\r\n",
        "        self.td_steps = 50  # Number of steps in the future to take into account for calculating the target value\r\n",
        "        self.PER = True  # Prioritized Replay (See paper appendix Training), select in priority the elements in the replay buffer which are unexpected for the network\r\n",
        "        self.PER_alpha = 0.5  # How much prioritization is used, 0 corresponding to the uniform case, paper suggests 1\r\n",
        "\r\n",
        "        # Reanalyze (See paper appendix Reanalyse)\r\n",
        "        self.use_last_model_value = True  # Use the last model to provide a fresher, stable n-step value (See paper appendix Reanalyze)\r\n",
        "        self.reanalyse_on_gpu = False\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        ### Adjust the self play / training ratio to avoid over/underfitting\r\n",
        "        self.self_play_delay = 0  # Number of seconds to wait after each played game\r\n",
        "        self.training_delay = 0  # Number of seconds to wait after each training step\r\n",
        "        self.ratio = 1.5  # Desired training steps per self played step ratio. Equivalent to a synchronous version, training can take much longer. Set it to None to disable it\r\n",
        "\r\n",
        "    def visit_softmax_temperature_fn(self, trained_steps):\r\n",
        "        \"\"\"\r\n",
        "        Parameter to alter the visit count distribution to ensure that the action selection becomes greedier as training progresses.\r\n",
        "        The smaller it is, the more likely the best action (ie with the highest visit count) is chosen.\r\n",
        "        Returns:\r\n",
        "            Positive float.\r\n",
        "        \"\"\"\r\n",
        "        if trained_steps < 0.5 * self.training_steps:\r\n",
        "            return 1.0\r\n",
        "        elif trained_steps < 0.75 * self.training_steps:\r\n",
        "            return 0.5\r\n",
        "        else:\r\n",
        "            return 0.25"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvLcWbdiyGnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a62aea2-2e93-4e55-aca9-a4107b456b49"
      },
      "source": [
        "# Begin the model's training\r\n",
        "from muzero_collab.muzero import MuZero\r\n",
        "import ray\r\n",
        "\r\n",
        "muzero = MuZero(\"battle\", BattleMuZeroConfig())\r\n",
        "\r\n",
        "if ray.is_initialized():\r\n",
        "  ray.shutdown()\r\n",
        "\r\n",
        "ray.init()\r\n",
        "muzero.train()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 03:15:44,726\tINFO worker.py:665 -- Calling ray.init() again after it has already been called.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shutting down workers...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 03:15:49,798\tINFO services.py:1174 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training...\n",
            "Run tensorboard --logdir ./results and go to http://localhost:6006/ to see in real time the training performance.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 03:15:54,296\tWARNING worker.py:1107 -- WARNING: 6 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 03:15:55,981\tWARNING worker.py:1107 -- WARNING: 8 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "Last test reward: 0.00. Training step: 0/5000. Played games: 0. Loss: 0.00\n",
            "\n",
            "Shutting down workers...\n",
            "\n",
            "\n",
            "Persisting replay buffer games to disk...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33a8or2xpQBE"
      },
      "source": [
        "# Download the logs folder as a tarball\r\n",
        "!tar czf results.tar.gz $LOGFOLDER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7JLCuxgVDAE"
      },
      "source": [
        "# Start tensorboard (if it works)\r\n",
        "%tensorboard --logdir muzero_collab/results"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}